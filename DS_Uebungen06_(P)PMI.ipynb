{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (P)PMI\n",
    "\n",
    "#### The roadmap \n",
    "\n",
    "1. load all the relevant data\n",
    "2. normalize data (tokenizing, puncutation removal, stemming)\n",
    "3. decide on your context and visualize your a word-context matrix\n",
    "4. use (P)PMI for smoothing your data\n",
    "5. Display the word-context matrix with new values\n",
    "\n",
    "The crucial question is: how do we want to build the word-context matrix? There is a number of possibilities how to approximate the \"context\". The context can be very large, i.e. the whole document can be considered as a context. The result would rather be a word-document-matrix. A smaller context could contain a word in the context of a sentence or in the context of a defined numbers of words left and/or right of the word in question (the number of co-occuring words is called _window size_). \n",
    "\n",
    "Main code reference:\n",
    "http://www.katrinerk.com/courses/computational-semantics-undergraduate/demo-the-building-blocks-of-a-distributional-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Source data\n",
    "\n",
    "The articles below are stored in the 'rex' folder.\n",
    "\n",
    "1. https://www.theguardian.com/science/2007/apr/13/uknews.taxonomy (rex_guardian)\n",
    "\n",
    "2. http://www.telegraph.co.uk/news/science/science-news/3340709/Chicken-is-T-rexs-closes-living-relative.html (rex_graph)\n",
    "\n",
    "3. http://www.independent.co.uk/news/science/tyrannosaurus-rex-was-more-like-a-chicken-than-a-crocodile-815417.html (rex_indy)\n",
    "\n",
    "4. https://www.livescience.com/1410-rex-related-chickens.html (rex_science)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "\n",
    "#nltk.download('punkt')   # Punkt Tokenizer Model\n",
    "#nltk.download('averaged_perceptron_tagger')  # Part-of-Speech Tokeniser\n",
    "#nltk.download(\"stopwords\") # Stopwords\n",
    "\n",
    "# modules for tokenization and removal of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# modules for stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace this directory with one on your own machine\n",
    "\n",
    "demo_dir = \"rex\"\n",
    "\n",
    "#import os\n",
    "\n",
    "# We iterate over the corpus files.\n",
    "\n",
    "# os.listdir lists the names of all files in a directory\n",
    "for filename in os.listdir(demo_dir):\n",
    "    if filename.endswith(\"txt\"):\n",
    "        print(\"reading file\", filename)\n",
    "        text = open(os.path.join(demo_dir, filename)).read()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_stemming(filtered):\n",
    "    stemmed = []\n",
    "    for f in filtered:\n",
    "        #stemmed.append(PorterStemmer().stem(f))\n",
    "        #stemmed.append(LancasterStemmer().stem(f))\n",
    "        stemmed.append(SnowballStemmer('english').stem(f))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# NLTK processing objects\n",
    "\n",
    "#import nltk\n",
    "\n",
    "#import string\n",
    "\n",
    "def preprocess(s):\n",
    "    # split up into words, lowercase, remove punctuation at beginning and end of word\n",
    "    remove_punctuation = [ w.lower().strip(string.punctuation) for w in s.split() ] # remove punctuation\n",
    "    remove_stopwords = [ w for w in remove_punctuation if w.lower() not in stopwords.words('english') ] # remove stopwords\n",
    "    return do_stemming(remove_stopwords)                        #include stemming\n",
    "\n",
    "\n",
    "# or like this:\n",
    "# def preprocess(s):\n",
    "#     words =  [ ]\n",
    "#     for w in s.split():\n",
    "#         word = w.lower()\n",
    "#         word = word.strip(string.punctuation)\n",
    "#         words.append(word)\n",
    "#     return words\n",
    "\n",
    "\n",
    "\n",
    "# use the function like this:\n",
    "preprocess(\"This is a test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#####################\n",
    "# Counting words:\n",
    "# We want to make a list of the N most frequent words in our corpus\n",
    "\n",
    "#import os\n",
    "\n",
    "def do_word_count(demo_dir, numdims):\n",
    "    # we store the counts in word_count\n",
    "    # using NLTK's FreqDist\n",
    "    word_count = nltk.FreqDist()\n",
    "   \n",
    "    # We iterate over the corpus files\n",
    "    for filename in os.listdir(demo_dir):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            print(\"reading file\", filename)\n",
    "            text = open(os.path.join(demo_dir, filename)).read()\n",
    "            word_count.update(preprocess(text))\n",
    "           \n",
    "    # keep_wordfreq is a list of (word, frequency) pairs\n",
    "    keep_wordfreq = word_count.most_common(numdims)\n",
    "    keep_these_words = [ w for w, freq in keep_wordfreq ]\n",
    "    # print(\"Target words:\\n\", keep_these_words, \"\\n\")\n",
    "   \n",
    "    return keep_these_words\n",
    "\n",
    "# or like this, without FreqDist:\n",
    "# def do_word_count(demo_dir, numdims):\n",
    "#     word_count = { }\n",
    "\n",
    "#     for filename in os.listdir(demo_dir):\n",
    "#         if filename.endswith(\"txt\"):\n",
    "#             print(\"reading file\", filename)\n",
    "#         text = open(os.path.join(demo_dir, filename)).read()\n",
    "#         for taggedword in preprocess(text):\n",
    "#             if taggedword not in word_count:\n",
    "#                 word_count[ taggedword ] = 0\n",
    "#             word_count[ taggedword ] += 1\n",
    "#\n",
    "#     def map_word_to_count(word): return word_count[ word ]\n",
    "#     keep_these_words = sorted(word_count.keys(), key = map_word_to_count)[:numdims]\n",
    "#    \n",
    "#     # print(\"Target words (and also dimensions):\\n\", keep_these_words, \"\\n\")\n",
    "#\n",
    "#     return keep_these_words\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "# run this:\n",
    "def test_wordcount():\n",
    "    print(\"Doing a frequency-based cutoff: keeping only the N most frequent context words.\")\n",
    "   \n",
    "    # with 10 dimensions\n",
    "    keepwords = do_word_count(demo_dir, 10)\n",
    "    print(\"Keeping only 10 dimensions, then I get:\", keepwords, \"\\n\")\n",
    "\n",
    "    # with 100 dimensions\n",
    "    keepwords = do_word_count(demo_dir, 100)\n",
    "    print(\"Keeping 100 dimensions, then I get:\", keepwords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_wordcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "# identifying context words for a narrow context window of 2 words on either side\n",
    "# of the target:\n",
    "# takes as input a sequence of words for counting.\n",
    "# For each word in the sequence, make 4 pairs:\n",
    "# (word, left neighbor of word), (word, left neighbor of left neighbor of word),\n",
    "# (word, right neighbor of word), (word, right neighbor of right neighbor of word),\n",
    "# so pair each word with all its context items in the context window.\n",
    "# Return a list of these pairs.\n",
    "def co_occurrences(wordsequence):\n",
    "    target_context_pairs = [ ]\n",
    "\n",
    "    # for a sequence of length N, count from 0 to N-1\n",
    "    for index in range(len(wordsequence) - 1):\n",
    "        # count that word[index] as a target co-occurred with the next word as a context item,\n",
    "        # and vice versa\n",
    "        target_context_pairs.append( (wordsequence[index], wordsequence[index+1]) )\n",
    "        target_context_pairs.append( (wordsequence[index+1], wordsequence[index]) )\n",
    "\n",
    "        if index + 2 < len(wordsequence):\n",
    "            # there is a word 2 words away\n",
    "            # count that word[index] as a target co-occurred with the but-next word as a context item,\n",
    "            # and vice versa\n",
    "            target_context_pairs.append( (wordsequence[index], wordsequence[index+2]) )\n",
    "            target_context_pairs.append( (wordsequence[index+2], wordsequence[index]) )\n",
    "\n",
    "    return target_context_pairs\n",
    "\n",
    "###\n",
    "# run this to test co-occurrences\n",
    "def test_cooccurrences():\n",
    "    text = \"\"\"You will not find Dr. Jekyll; he is from home,\" replied Mr. Hyde\"\"\"\n",
    "    print(\"Testing the function that pairs up each target word with its context words.\")\n",
    "    print(\"Original text:\", text, \"\\n\")\n",
    "\n",
    "    words = preprocess(text)\n",
    "    cooc = co_occurrences(words)\n",
    "    print(\"These are the target/context pairs:\", cooc, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_cooccurrences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##\n",
    "# We will need the function make_word_index below.\n",
    "# It maps each word that we want to keep around as a context item\n",
    "# to an index, which will be its place in the table of counts,\n",
    "# that is, its dimension in the space\n",
    "\n",
    "def make_word_index(keep_these_words):\n",
    "    # make an index that maps words from 'keep_these_words' to their index\n",
    "    word_index = { }\n",
    "    for index, word in enumerate(keep_these_words):\n",
    "        word_index[ word ] = index\n",
    "\n",
    "    return word_index\n",
    "\n",
    "#import numpy #as np\n",
    "\n",
    "# read all files in demo_dir, and compute a counts vector\n",
    "# of length numdims for each relevant word.\n",
    "# The function takes as input also a mapping word_index from relevant words\n",
    "# to their dimension, from which we derive a set relevant_words.\n",
    "# This function reads the texts one sentence at a time.\n",
    "# In each sentence, it identifies context words in the window\n",
    "# defined by co_occurrences(), and stores them if both the target\n",
    "# and its context words are relevant_words\n",
    "\n",
    "def make_space(demo_dir, word_index, numdims):\n",
    "\n",
    "    # relevant words: those that have an entry in word_index\n",
    "    relevant_words = set(word_index.keys())\n",
    "\n",
    "    # space: a mapping from relevant_words to an array of integers (raw counts)\n",
    "    space = { }\n",
    "    # fill the space with all zeros.\n",
    "    for word in relevant_words:\n",
    "        space[ word ] = np.zeros(numdims, dtype = np.int)\n",
    "\n",
    "    ##\n",
    "    # Design decision: We want to take sentence boundaries into account\n",
    "    # when computing distributional representations.\n",
    "    # So we need to detect sentence boundaries first.\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    # We iterate over the corpus files\n",
    "    # and count word co-occurrences in a window of 2\n",
    "    for filename in os.listdir(demo_dir):\n",
    "        if filename.endswith(\"txt\"):\n",
    "            print(\"reading file\", filename)\n",
    "            # read the text\n",
    "            text = open(os.path.join(demo_dir, filename)).read()\n",
    "            # split the text into sentences\n",
    "            sentences = sent_detector.tokenize(text)\n",
    "            # process one sentence at a time\n",
    "            for sentence in sentences:\n",
    "                words = preprocess(sentence)\n",
    "\n",
    "                # determine pairs of co-occurrences to count,\n",
    "                # and store them in the matrix\n",
    "                for target, cxitem in co_occurrences(words):\n",
    "                    # are these two words relevant?\n",
    "                    if target in relevant_words and cxitem in relevant_words:\n",
    "                        # what is the row for this context item?\n",
    "                        cxitem_index = word_index[ cxitem]\n",
    "                        # now count\n",
    "                        space[ target ][cxitem_index] += 1\n",
    "\n",
    "\n",
    "    return space\n",
    "\n",
    "###\n",
    "# run this\n",
    "def test_space():\n",
    "    numdims = 10 # change the dimensions at free will!\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "   \n",
    "    print(\"word index:\")\n",
    "    for word in words_in_order:\n",
    "        print(word, wi[word], end= \" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "   \n",
    "    print(\"some words from the space\")\n",
    "    for w in words_in_order[:10]:\n",
    "        print(w,  space[w], \"\\n\")\n",
    "        \n",
    "    return space, words_in_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "space, words_in_order = test_space()\n",
    "\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "word_context_matrix = pd.DataFrame.from_dict(space, orient='index')\n",
    "word_context_matrix.columns = words_in_order\n",
    "\n",
    "print(word_context_matrix)\n",
    "#print(words_in_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute (P)PMI\n",
    "\n",
    "$PMI(w_{1},w_{2}) = log(\\frac{P(w_{1},w_{2})}{P(w_{1})P(w_{2})})$\n",
    "\n",
    "$PPMI(w_{1},w_{2}) = max(log(\\frac{P(w_{1},w_{2})}{P(w_{1})P(w_{2})}),0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# transform the space using positive(?) pointwise mutual information\n",
    "\n",
    "# target t, dimension value c, then\n",
    "# PMI(t, c) = log ( P(t, c) / (P(t) P(c)) )\n",
    "# where\n",
    "# P(t, c) = #(t, c) / #(_, _)\n",
    "# P(t) = #(t, _) / #(_, _)\n",
    "# P(c) = #(_, c) / #(_, _)\n",
    "#\n",
    "# PPMI(t, c) =   PMI(t, c) if PMI(t, c) > 0\n",
    "#                0 else\n",
    "def ppmi_transform(space, word_index):\n",
    "    # #(t, _): for each target word, sum up all its counts.\n",
    "    # row_sums is a dictionary mapping from target words to row sums\n",
    "    row_sums = { }\n",
    "    for word in space.keys():\n",
    "        row_sums[word] = space[word].sum()\n",
    "\n",
    "    # #(_, c): for each context word, sum up all its counts\n",
    "    # This should be the same as #(t, _) because the set of targets\n",
    "    # is the same as the set of contexts.\n",
    "    # col_sums is a dictionary mapping from context word indices to column sums\n",
    "    col_sums = { }\n",
    "    for index in word_index.values():\n",
    "        col_sums[ index ] = sum( [ vector[ index ] for vector in space.values() ])\n",
    "\n",
    "    # sanity check: row sums same as column sums?\n",
    "    for word in space.keys():\n",
    "        if row_sums[word] != col_sums[ word_index[word]]:\n",
    "            print(\"whoops, failed sanity check for\", word, row_sums[word], col_sums[word_index[word]])\n",
    "   \n",
    "    # #(_, _): overall count of occurrences. sum of all row_sums\n",
    "    all_sums = sum(row_sums.values())\n",
    "\n",
    "    # if all_sums is zero, there's nothing we can do\n",
    "    # because we then cannot divide by #(_, _)\n",
    "    if all_sums == 0:\n",
    "        print(\"completely empty space, returning it unchanged\")\n",
    "        return space\n",
    "\n",
    "    # P(t) = #(t, _) / #(_, _)\n",
    "    p_t = { }\n",
    "    for word in space.keys():\n",
    "        p_t[ word ] = row_sums[ word ] / all_sums\n",
    "\n",
    "    # P(c) = #(_, c) / #(_, _)\n",
    "    p_c = { }\n",
    "    for index in col_sums.keys():\n",
    "        p_c[ index ] = col_sums[ index ] / all_sums\n",
    "\n",
    "    # ppmi_space: a mapping from words to vectors of values\n",
    "    ppmi_space = { }\n",
    "    # first we map from words to values P(t, c)\n",
    "    for word in space.keys():\n",
    "        ppmi_space[ word ] = space[ word ] / all_sums\n",
    "    # divide each entry by P(t)\n",
    "    for word in space.keys():\n",
    "        if p_t[ word ] == 0:\n",
    "            # I haven't seen this word ever, so I cannot\n",
    "            # divide by P(t). But the whole entry for this word\n",
    "            # should be 0's, so leave as is.\n",
    "            pass\n",
    "        else:\n",
    "            ppmi_space[ word ] = ppmi_space[ word ] / p_t[ word ]\n",
    "    # divide each entry by P(c)\n",
    "    for index in p_c.keys():\n",
    "        if p_c[ index ] == 0:\n",
    "            # I haven't seen this context item ever,\n",
    "            # so I cannot divide by P(c).\n",
    "            # But every target word will have an entry of 0.0\n",
    "            # on this column, so nothing more to do.\n",
    "            pass\n",
    "        else:\n",
    "            for word in space.keys():\n",
    "                ppmi_space[ word ][index] = ppmi_space[ word][index] / p_c[ index ]\n",
    "               \n",
    "    # take the logarithm, ignore entries that are zero\n",
    "    for word in space.keys():\n",
    "        with np.errstate(divide=\"ignore\",invalid=\"ignore\"):\n",
    "            ppmi_space[ word ] = np.log(ppmi_space[ word ])\n",
    "           \n",
    "\n",
    "    # turn negative numbers to zero\n",
    "    for word in space.keys():\n",
    "        ppmi_space[word] = np.maximum(ppmi_space[word], 0.0)\n",
    "\n",
    "    return ppmi_space\n",
    "\n",
    "###\n",
    "# run this:\n",
    "def test_ppmispace():\n",
    "    numdims = 10\n",
    "    # which words to use as targets and context words?\n",
    "    ktw = do_word_count(demo_dir, numdims)\n",
    "    # mapping words to an index, which will be their column\n",
    "    # in the table of counts\n",
    "    wi = make_word_index(ktw)\n",
    "    words_in_order = sorted(wi.keys(), key=lambda w:wi[w])\n",
    "   \n",
    "    print(\"word index:\")\n",
    "    for word in words_in_order:\n",
    "        print(word, wi[word], end=\" \")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    space = make_space(demo_dir, wi, numdims)\n",
    "    ppmispace = ppmi_transform(space, wi)\n",
    "    \n",
    "    print(\"some raw counts vectors and some ppmi vectors\")\n",
    "    for w in words_in_order[:10]:\n",
    "        print(\"---------\", \"\\n\", w)\n",
    "        print(\"raw\", space[w])\n",
    "        # for the PPMI space, we're rounding to 2 digits after the floating point\n",
    "        print(\"ppmi\", np.round(ppmispace[w], 2), \"\\n\")\n",
    "        \n",
    "    return space, ppmispace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ppmispace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space, ppmispace = test_ppmispace()\n",
    "\n",
    "PMI_matrix = pd.DataFrame.from_dict(ppmispace, orient='index')\n",
    "PMI_matrix.columns = words_in_order\n",
    "\n",
    "print(PMI_matrix)\n",
    "#print(words_in_order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
