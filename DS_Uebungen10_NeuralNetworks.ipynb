{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Networks with Spiral Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercise is borrowed from https://cs231n.github.io/neural-networks-case-study/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Spiral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0,1,N) # radius\n",
    "    t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-1.5,1.5])\n",
    "plt.ylim([-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "<img src=\"img/activation_functions.png\" title=\"A collection of activation functions\"/>\n",
    "Source: https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x, dforward):\n",
    "    t = sigmoid(x)\n",
    "    return np.multiply(dforward, t * (1.0 - t))\n",
    "\n",
    "def ReLU(X, W, b):\n",
    "    return np.maximum(0, np.dot(X, W) + b)\n",
    "    #pass\n",
    "\n",
    "def tanh(x):\n",
    "    #pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions\n",
    "\n",
    "A new activation function is used here: softmax. It is basically used to handle multiple classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural layers\n",
    "def fn_layer1(X, W1, b1):\n",
    "    layer1_f = np.dot(X, W1) + b1\n",
    "    #hidden_layer = sigmoid(layer1_f) # Parameter to play around with activation functions\n",
    "    #hidden_layer = ReLU(X, W1, b1)\n",
    "    #hidden_layer = tanh(layer1_f)\n",
    "    hidden_layer = layer1_f\n",
    "        \n",
    "    return layer1_f, hidden_layer\n",
    "\n",
    "\n",
    "def fn_layer2(hidden_layer, W2, b2):\n",
    "    f = np.dot(hidden_layer, W2) + b2\n",
    "    \n",
    "    return f\n",
    "\n",
    "\n",
    "def softmax(f):\n",
    "    exp_f = np.exp(f)\n",
    "    probs = exp_f / np.sum(exp_f, axis=1, keepdims=True) # [N x K]\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "def loss(X, W1, b1, W2, b2, y, reg=1e-3):\n",
    "    # compute the class probabilities\n",
    "    layer1_f, hidden_layer = fn_layer1(X, W1, b1)\n",
    "    probs = softmax(fn_layer2(hidden_layer, W2, b2))\n",
    "\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)\n",
    "    \n",
    "    # compute total loss\n",
    "    loss = data_loss + reg_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/backprop.jpg\" title=\"Example of a neural network\"/>\n",
    "Source: https://www.techleer.com/articles/242-backpropagation-a-supervised-learning-neural-network-method/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradients(X, W1, b1, W2, b2, y):\n",
    "    # compute the class probabilities\n",
    "    layer1_f, hidden_layer = fn_layer1(X, W1, b1) # hidden_layer is already sigmoid / ReLU / tanh etc. activated\n",
    "    probs = softmax(fn_layer2(hidden_layer, W2, b2)) # get the probabilities\n",
    "\n",
    "\n",
    "    # compute the gradient of function\n",
    "    df = probs\n",
    "    df[range(num_examples),y] -= 1\n",
    "    df /= num_examples\n",
    "    \n",
    "    # backpropagate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, df)\n",
    "    db2 = np.sum(df, axis=0, keepdims=True)\n",
    "    \n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(df, W2.T)\n",
    "    \n",
    "    # backprop the sigmoid, ReLU non-linearity, tanh    # change parameteres here!\n",
    "    #dhidden[hidden_layer <= 0] = 0            # derived ReLU function\n",
    "    #dhidden = dsigmoid(layer1_f, dhidden)     # derived sigmoid function\n",
    "    #dhidden = dtanh(layer1_f)                 # derived tanh function\n",
    "    \n",
    "    # finally into W1 and b1\n",
    "    dW1 = np.dot(X.T, dhidden)\n",
    "    db1 = np.sum(dhidden, axis=0, keepdims=True)\n",
    "    \n",
    "    # add regularization gradient contribution\n",
    "    dW2 += reg * W2\n",
    "    dW1 += reg * W1\n",
    "    \n",
    "    return (dW1, db1, dW2, db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the parameters and perform the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer; can be set individually\n",
    "W1 = 0.01 * np.random.randn(D,h)\n",
    "b1 = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(10000):\n",
    "    # Calculate average loss\n",
    "    avg_loss = loss(X, W1, b1, W2, b2, y, reg=reg)\n",
    "    if i % 1000 == 0:\n",
    "        print (\"iteration %d: loss %f\" % (i, avg_loss))\n",
    "\n",
    "    # Compute the gradients\n",
    "    dW1, db1, dW2, db2 = gradients(X, W1, b1, W2, b2, y)\n",
    "\n",
    "    # perform parameter updates\n",
    "    W1 += -step_size * dW1\n",
    "    b1 += -step_size * db1\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the accuracy of the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "layer1_f, hidden_layer = fn_layer1(X, W1, b1)\n",
    "scores = fn_layer2(hidden_layer, W2, b2)\n",
    "\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print ('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this one doesn't work, try the second!\n",
    "\n",
    "colors = ['r','y','b']\n",
    "nx,ny = 500,500\n",
    "xx,yy = np.meshgrid(np.linspace(-1.5, 1.5, nx), np.linspace(-1, 1, ny))\n",
    "x_grid = np.hstack((xx.flatten().reshape(nx*ny,1), yy.flatten().reshape(nx*ny,1)))\n",
    "\n",
    "layer1_f, hidden_layer = fn_layer1(x_grid, W1, b1)\n",
    "layer2_f = fn_layer2(hidden_layer, W2, b2)\n",
    "\n",
    "y_pred = np.argmax(layer2_f, axis=1)\n",
    "\n",
    "plt.scatter(xx, yy, c=y_pred, s=40, marker='s', edgecolor='none', cmap=plt.cm.Spectral)\n",
    "plt.scatter(X[:,0], X[:, 1], c=[colors[l] for l in y], s=40)\n",
    "plt.axis('equal')\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.xlim([-1.5,1.5])\n",
    "plt.ylim([-1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], W1) + b1), W2) + b2\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
