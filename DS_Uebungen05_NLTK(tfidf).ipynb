{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics (Übung): NLTK (tf * idf)\n",
    "\n",
    "NLTK is one of the largest Python libraries for Natural Language Processing. It was written in particular for educational purposes in computational linguistics and information retrieval. The developer Steven Bird, Ewan Klein, Edward Loper developed NLTK as a open source project the first time for Python 2.6.\n",
    "\n",
    "Access to NLTK: http://www.nltk.org/\n",
    "\n",
    "## Installation\n",
    "If you use Jupyter Notebook, NLTK usually is pre-installed. Nevertheless, in the case that something went wrong during installation, just type the commands below in a debian-based (Ubuntu) OS. Numpy is also a module which is used by NLTK.\n",
    "\n",
    "    >>> sudo pip3 install -U nltk\n",
    "\n",
    "    >>> sudo pip3 install -U numpy\n",
    "    \n",
    "For the the missing modules in Anaconda, just type in:\n",
    "\n",
    "    >>> conda install nltk\n",
    "\n",
    "    >>> conda install numpy\n",
    "    \n",
    "You can check, whether the installation was successful, by executing \"import nltk\" in your python shell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The roadmap of this session\n",
    "\n",
    "1. Import and include text data\n",
    "2. Tokenize raw data\n",
    "3. Remove stopwords\n",
    "4. Display a term-document matrix\n",
    "5. Apply tf'idf\n",
    "6. Display the term-document matrix with smoothed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# If you are interested in large corpora, you can access and download pre-set corpora from NLTK. \n",
    "nltk.download(\"reuters\") # download corpora\n",
    "\n",
    "# You can use one specific corpus ressource\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Access different kind of information of the corpus by the following commands\n",
    "\n",
    "brown.fileids() # corpora data\n",
    "rawtext = brown.raw() # raw data\n",
    "words = brown.words() # words from the data\n",
    "sentences = brown.sentences() # sentences from the data\n",
    "text1.concordance(\"whale\") # pretty print the concordances\n",
    "text1.similar(\"whale\") # calculate similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with NLTK demands a couple of imported modules. The most important ones are loaded at once below. First you are required to download the additional data before applying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')   # Punkt Tokenizer Model\n",
    "nltk.download('averaged_perceptron_tagger')  # Part-of-Speech Tokeniser\n",
    "nltk.download(\"stopwords\") # Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "# modules for tokenization and removal of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# modules for stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need some text data to process. Consider the following three documents in our dummy set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ulysses = \"\"\"Mrkgnao! the cat said loudly. She blinked up out of her avid shameclosing eyes, mewing \\\n",
    "plaintively and long, showing him her milkwhite teeth. He watched the dark eyeslits narrowing \\\n",
    "with greed till her eyes were green stones. Then he went to the dresser, took the jug Hanlon's\\\n",
    "milkman had just filled for him, poured warmbubbled milk on a saucer and set it slowly on the floor.\\\n",
    "— Gurrhr! she cried, running to lap.\"\"\"\n",
    "\n",
    "hungry_cat = \"\"\"The hungry cat\n",
    "A hungry cat is looking for something to eat. She sees a little grey mouse sitting near his house. \\\n",
    "- I want to catch that little mouse, - says the hungry cat. \\\n",
    "She sits down and begins to cry \"mew, mew, mew\". \\\n",
    "The little grey mouse jumps up to run into his house, but the cat sits still and mews again. \\\n",
    "- She is sitting still, - thinks the mouse. \\\n",
    "- She doesn't want to catch me. I shall not run away. \\\n",
    "- Mew, mew, mew, - says the cat again. \\\n",
    "- Why are you crying? - asks the mouse. \\\n",
    "- See, I have a penny in my hand. \\\n",
    "- Good, you are lucky. That's nothing to cry about, - says the mouse. \\\n",
    "The hungry cat comes nearer. \\\n",
    "- Oh, little mouse, I shall get some meat with the penny. I shall cook it and have it for supper. \\\n",
    "- Good, you are lucky. That's nothing to cry about. \\\n",
    "The hungry cat comes nearer and nearer. \\\n",
    "- There lives a hungry dog in this house. He will eat all the meat. \\\n",
    "- Poor Pussy, - says the mouse. - What will you eat then? \\\n",
    "- You, - cries the cat and jumps at the little grey mouse. \\\n",
    "But the mouse is too quick. He jumps into his little house before the cat can say another \"mew\". \\\n",
    "- No, no, sly Pussy, - says the mouse. - You will not eat me. You must first catch me.\"\"\"\n",
    "\n",
    "bell_the_cat = \"\"\"There was a grocery shop in a town. Plenty of mice lived in that grocery shop. Food was in plenty for them. They ate everything and spoiled all the bags. They also wasted the bread, biscuits and fruits of the shop.  \\\n",
    "The grocer got really worried. So, he thought \"I should buy a cat and let it stay at the grocery. Only then I can save my things.\" \\\n",
    "He bought a nice, big fat cat and let him stay there. The cat had a nice time hunting the mice and killing them. The mice could not move freely now. They were afraid that anytime the cat would eat them up. \\\n",
    "The mice wanted to do something. They held a meeting and all of them tweeted \"We must get rid of the cat. Can someone give a suggestion\"? \\\n",
    "All the mice sat and brooded. A smart looking mouse stood up and said, \"The cat moves softly. That is the problem. If we can tie a bell around her neck, then things will be fine. We can know the movements of the cat\". \\\n",
    "“Yes, that is answer,\" stated all the mice. An old mouse slowly stood up and asked, \"Who would tie the bell?\" After some moments there was no one there to answer this question. \\\n",
    "MORAL : Empty solutions are of no worth.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing\n",
    "\n",
    "NLTK has an inbuild tokenizer which seperates in sentences (sent_tokenize()) or in words (word_tokenize()). Actually, there is a number of possibilities to tokenize the raw text, depending on the desired result.\n",
    "\n",
    "The next step is important, so that the one large string of each raw data gets cut into single words. In our approach, remove all the punctuation with a regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#doc = nltk.word_tokenize(ulysses)\n",
    "#print(doc)\n",
    "#for s in doc:\n",
    "#    print(\">\",s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ulysses = re.findall(r\"(?:[A-Z]\\.)+|\\w+(?:[']\\w+)*|\\$?\\d+(?:\\.\\d+)?%?\", ulysses)\n",
    "hungry_cat = re.findall(r\"(?:[A-Z]\\.)+|\\w+(?:[']\\w+)*|\\$?\\d+(?:\\.\\d+)?%?\", hungry_cat)\n",
    "bell_the_cat = re.findall(r\"(?:[A-Z]\\.)+|\\w+(?:[']\\w+)*|\\$?\\d+(?:\\.\\d+)?%?\", bell_the_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we imported a list of stopword. If you wish, you can print them out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove stopwords from the data, apply the next command. __Task:__ Do the same steps for the other two documents!\n",
    "\n",
    "Note: the larger the data, the longer it takes to process. On larger texts it can takes a couples of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bellcat_withoutStopwords = [w for w in bell_the_cat if w.lower() not in stopwords.words('english') ]\n",
    "ulysses_withoutStopwords = [w for w in ulysses if w.lower() not in stopwords.words('english') ]\n",
    "hungrycat_withoutStopwords = [w for w in hungry_cat if w.lower() not in stopwords.words('english') ]\n",
    "print(bellcat_withoutStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instead of the regex, the code below would equally remove all the punctuation.\n",
    "\n",
    "#bell_the_cat = [w for w in bell_the_cat if w not in string.punctuation]\n",
    "#punctCombo = [c+\"\\\"\" for c in string.punctuation ]+ [\"\\\"\"+c for c in string.punctuation ]\n",
    "#bell_the_cat = [w for w in bell_the_cat if w not in punctCombo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# If you print out the length of both texts, you will realize that the size was reduced due to sorting out stopwords\n",
    "print(len(bell_the_cat))\n",
    "print(len(bellcat_withoutStopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use the data, plot, and see your first results. :-)\n",
    "\n",
    "fdist_catuly = nltk.FreqDist(ulysses_withoutStopwords)\n",
    "fdist_cat1 = nltk.FreqDist(hungrycat_withoutStopwords)\n",
    "fdist_cat2 = nltk.FreqDist(bellcat_withoutStopwords)\n",
    "\n",
    "fdist_catuly.plot(25, cumulative=False)\n",
    "fdist_cat1.plot(25, cumulative=False)\n",
    "fdist_cat2.plot(25, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatizing\n",
    "\n",
    "Stemming and lemmatizing are two similar methods to standarize your data. Stemming reduces words to its stems by defined rules. Lemmatizer need an additional lexicon for this task, but the results are 'cleaner'. For our purposes a stemmer ist perfectly fine.\n",
    "\n",
    "Reminder: Your tokenized data without stopwords are bellcat_withoutStopwords, ulysses_withoutStopwords, hungrycat_withoutStopwords.\n",
    "\n",
    "__Task:__ Play with the other documents and the different stemmers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You are free to choose your stemmer! Just un/comment the correct ones.\n",
    "\n",
    "def do_stemming(filtered):\n",
    "    stemmed = []\n",
    "    for f in filtered:\n",
    "        stemmed.append(PorterStemmer().stem(f))\n",
    "        #stemmed.append(LancasterStemmer().stem(f))\n",
    "        #stemmed.append(SnowballStemmer('english').stem(f))\n",
    "    return stemmed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"tokens = %s \\n\" %(bellcat_withoutStopwords))\n",
    "\n",
    "    stemmed_tokens = do_stemming(bellcat_withoutStopwords)\n",
    "    print(\"stemmed_tokens = %s \\n\" %stemmed_tokens)\n",
    "\n",
    "    result = dict(zip(bellcat_withoutStopwords, stemmed_tokens))\n",
    "    for element, stemmed in result.items():\n",
    "        print(\"stemmed token: {} \\t {}\".format(element, stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ulysses_stemmed = do_stemming(ulysses_withoutStopwords)\n",
    "hungrycat_stemmed = do_stemming(hungrycat_withoutStopwords)\n",
    "bellcat_stemmed = do_stemming(bellcat_withoutStopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowing some sklearn library enables us to print the word-document matrix!\n",
    "\n",
    "__Task:__ The data is not stemmed. What will the matrix look like after stemming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [str(ulysses_withoutStopwords), str(hungrycat_withoutStopwords), str(bellcat_withoutStopwords)]\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(docs)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "df = pd.DataFrame(X.toarray().transpose(), index=vec.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency * Inverse Document Frequency\n",
    "\n",
    "__Term Frequency _tf(w,d)_:__ number of times a word _w_ appears in a document _d_.\n",
    "\n",
    "<img src=\"img/tf.png\" align=\"left\" title=\"Source: http://www.akbarian.org/notes/text-mining-nlp-python/\"/>\n",
    "\n",
    "<img src=\"img/idf.png\" align=\"left\" title=\"Source: http://www.akbarian.org/notes/text-mining-nlp-python/\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "tfidf.toarray().transpose() \n",
    "\n",
    "\n",
    "dtfidf = pd.DataFrame(tfidf.toarray().transpose(), index=vec.get_feature_names())\n",
    "print(dtfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some further references:\n",
    "\n",
    "https://github.com/edbullen/nltk\n",
    "\n",
    "http://www.nltk.org/\n",
    "\n",
    "http://www.akbarian.org/notes/text-mining-nlp-python/\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
